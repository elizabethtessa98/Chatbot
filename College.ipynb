{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zolJNsBe3UfY"
      },
      "source": [
        "A chatbot is an artificial intelligence (AI) software that can simulate a conversation (or a chat) with a user in natural language through messaging applications, websites, mobile apps or through the telephone. Why are chatbots important? A chatbot is often described as one of the most advanced and promising expressions of interaction between humans and machines. However, from a technological point of view, a chatbot only represents the natural evolution of a Question Answering system leveraging Natural Language Processing (NLP). Formulating responses to questions in natural language is one of the most typical Examples of Natural Language Processing applied in various enterprisesâ€™ end-use applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1Mjqx1QTkiL"
      },
      "source": [
        "# Chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **College**"
      ],
      "metadata": {
        "id": "-p8SXz1sg7YO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4 as bs                  #beautifulsoup4 library for parsing the webpage\n",
        "import urllib.request             #the urllib library for connection to a remote webpage\n",
        "import re                         #the re  library for performing regex operation\n",
        "\n",
        "import nltk              #the nltk  library for natural language processing\n",
        "import numpy as np       #the numpy  library for basic array operations\n",
        "import random            #the random  library is used for random number generation.\n",
        "import string            #the string  library is used for string manipulation."
      ],
      "metadata": {
        "id": "0NEVQaiWhDQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Scrapping the article from the wiki page\n",
        "\n",
        "rawdata = urllib.request.urlopen('https://en.wikipedia.org/wiki/College')\n",
        "rawdata = rawdata.read()\n",
        "\n",
        "html_data = bs.BeautifulSoup(rawdata,'html.parser')\n",
        "\n",
        "all_paragraphs =html_data.find_all('p')\n",
        "\n",
        "article_content = \"\"\n",
        "\n",
        "for p in all_paragraphs:\n",
        "    article_content += p.text\n",
        "\n",
        "article_content =  article_content.lower() #turning all words to lowercase"
      ],
      "metadata": {
        "id": "hrLDsd2ZhIZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#removing numbers from our dataset and replace multiple empty spaces with single space.\n",
        "#(This step is optional, you can skip it)\n",
        "\n",
        "article_content = re.sub(r'\\[[0-9]*\\]', ' ', article_content )\n",
        "article_content = re.sub(r'\\s+', ' ', article_content )"
      ],
      "metadata": {
        "id": "9kRJp-9bhK6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizing the article into sentences:\n",
        "\n",
        "sentence_list = nltk.sent_tokenize(article_content)\n",
        "article_words= nltk.word_tokenize(article_content )"
      ],
      "metadata": {
        "id": "i8yzuKXYhNI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "def LemmatizeWords(words):\n",
        "    return [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "remove_punctuation= dict((ord(punctuation), None) for punctuation in string.punctuation)\n",
        "\n",
        "def RemovePunctuations(text):\n",
        "    return LemmatizeWords(nltk.word_tokenize(text.lower().translate(remove_punctuation)))"
      ],
      "metadata": {
        "id": "fJeDu6-2hPVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Greetings\n",
        "\n",
        "greeting_input_texts = (\"hey\", \"hi\", \"hello\", \"morning\", \"evening\",\"greetings\",\"afternoon\",)\n",
        "greeting_replie_texts = [\"hey\", \"hey, how are you?\", \"ðŸ‘‹ how may i help you?\", \"hello there\", \"hello\", \"Welcome, how are you\"]\n",
        "\n",
        "def reply_greeting(text):\n",
        "\n",
        "    for word in text.split():\n",
        "        if word.lower() in greeting_input_texts:\n",
        "            return random.choice(greeting_replie_texts)"
      ],
      "metadata": {
        "id": "-hAL1SJBhR2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "4N-z9aKjhVL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def give_reply(user_input):\n",
        "    chatbot_response=''\n",
        "    sentence_list.append(user_input)\n",
        "    word_vectors = TfidfVectorizer(tokenizer=RemovePunctuations, stop_words='english')\n",
        "    vecrorized_words = word_vectors.fit_transform(sentence_list)\n",
        "    similarity_values = cosine_similarity(vecrorized_words[-1], vecrorized_words)\n",
        "    similar_sentence_number =similarity_values.argsort()[0][-2]\n",
        "    similar_vectors = similarity_values.flatten()\n",
        "    similar_vectors.sort()\n",
        "    matched_vector = similar_vectors[-2]\n",
        "    if(matched_vector ==0):\n",
        "        chatbot_response=chatbot_response+\"I am sorry! I don't understand you. ask me something else\"\n",
        "        return chatbot_response\n",
        "    else:\n",
        "        chatbot_response = chatbot_response +sentence_list[similar_sentence_number]\n",
        "        return chatbot_response"
      ],
      "metadata": {
        "id": "lfb7v8ahhY6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "continue_discussion=True\n",
        "print(\"Hello, my name is Prince, I will answer your questions about global warming: say bye to end conversation.\")\n",
        "while(continue_discussion==True):\n",
        "    user_input = input()\n",
        "    user_input = user_input .lower()\n",
        "    if(user_input !='bye'):\n",
        "        if(user_input =='thanks' or user_input =='thank you very much'  or user_input =='thank you'):\n",
        "            continue_discussion=False\n",
        "            print(\"Prince: Most welcome\")\n",
        "        else:\n",
        "            if(reply_greeting(user_input)!=None):\n",
        "                print(\"Chatbot: \"+reply_greeting(user_input))\n",
        "            else:\n",
        "                print(\"Prince: \",end=\"\")\n",
        "                print(give_reply(user_input))\n",
        "                sentence_list.remove(user_input)\n",
        "    else:\n",
        "        continue_discussion=False\n",
        "        print(\"Prince: Take care, bye ..\")"
      ],
      "metadata": {
        "id": "jNcWL-4OhbgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mrIuhdCL13t"
      },
      "source": [
        "\n",
        "# Galgotias ChatBot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5AuNzrGez0W"
      },
      "outputs": [],
      "source": [
        "import bs4 as bs                  #beautifulsoup4 library for parsing the webpage\n",
        "import urllib.request             #the urllib library for connection to a remote webpage\n",
        "import re                         #the re  library for performing regex operation\n",
        "\n",
        "import nltk              #the nltk  library for natural language processing\n",
        "import numpy as np       #the numpy  library for basic array operations\n",
        "import random            #the random  library is used for random number generation.\n",
        "import string            #the string  library is used for string manipulation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7D-p5eqzLzCX"
      },
      "outputs": [],
      "source": [
        "#Scrapping the article from the wiki page\n",
        "\n",
        "rawdata = urllib.request.urlopen('https://en.wikipedia.org/wiki/Galgotias_College')\n",
        "rawdata = rawdata.read()\n",
        "\n",
        "html_data = bs.BeautifulSoup(rawdata,'html.parser')\n",
        "\n",
        "all_paragraphs =html_data.find_all('p')\n",
        "\n",
        "article_content = \"\"\n",
        "\n",
        "for p in all_paragraphs:\n",
        "    article_content += p.text\n",
        "\n",
        "article_content =  article_content.lower() #turning all words to lowercase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1WLN-ETL7CW"
      },
      "outputs": [],
      "source": [
        "#removing numbers from our dataset and replace multiple empty spaces with single space.\n",
        "#(This step is optional, you can skip it)\n",
        "\n",
        "article_content = re.sub(r'\\[[0-9]*\\]', ' ', article_content )\n",
        "article_content = re.sub(r'\\s+', ' ', article_content )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CGPFbcjL_QW"
      },
      "outputs": [],
      "source": [
        "#tokenizing the article into sentences:\n",
        "\n",
        "sentence_list = nltk.sent_tokenize(article_content)\n",
        "article_words= nltk.word_tokenize(article_content )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAugoGzNMApd"
      },
      "outputs": [],
      "source": [
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "def LemmatizeWords(words):\n",
        "    return [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "remove_punctuation= dict((ord(punctuation), None) for punctuation in string.punctuation)\n",
        "\n",
        "def RemovePunctuations(text):\n",
        "    return LemmatizeWords(nltk.word_tokenize(text.lower().translate(remove_punctuation)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab2kE7qaMCPv"
      },
      "outputs": [],
      "source": [
        "#Creating Greetings\n",
        "\n",
        "greeting_input_texts = (\"hey\", \"hi\", \"hello\", \"morning\", \"evening\",\"greetings\",\"afternoon\",)\n",
        "greeting_replie_texts = [\"hey\", \"hey, how are you?\", \"ðŸ‘‹ how may i help you?\", \"hello there\", \"hello\", \"Welcome, how are you\"]\n",
        "\n",
        "def reply_greeting(text):\n",
        "\n",
        "    for word in text.split():\n",
        "        if word.lower() in greeting_input_texts:\n",
        "            return random.choice(greeting_replie_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJ1IAFkrMD3c"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YR2Dlv8dMGYN"
      },
      "outputs": [],
      "source": [
        "def give_reply(user_input):\n",
        "    chatbot_response=''\n",
        "    sentence_list.append(user_input)\n",
        "    word_vectors = TfidfVectorizer(tokenizer=RemovePunctuations, stop_words='english')\n",
        "    vecrorized_words = word_vectors.fit_transform(sentence_list)\n",
        "    similarity_values = cosine_similarity(vecrorized_words[-1], vecrorized_words)\n",
        "    similar_sentence_number =similarity_values.argsort()[0][-2]\n",
        "    similar_vectors = similarity_values.flatten()\n",
        "    similar_vectors.sort()\n",
        "    matched_vector = similar_vectors[-2]\n",
        "    if(matched_vector ==0):\n",
        "        chatbot_response=chatbot_response+\"I am sorry! I don't understand you. ask me something else\"\n",
        "        return chatbot_response\n",
        "    else:\n",
        "        chatbot_response = chatbot_response +sentence_list[similar_sentence_number]\n",
        "        return chatbot_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_LuJcfBMIKW"
      },
      "outputs": [],
      "source": [
        "\n",
        "continue_discussion=True\n",
        "print(\"Hello, my name is Prince, I will answer your questions about global warming: say bye to end conversation.\")\n",
        "while(continue_discussion==True):\n",
        "    user_input = input()\n",
        "    user_input = user_input .lower()\n",
        "    if(user_input !='bye'):\n",
        "        if(user_input =='thanks' or user_input =='thank you very much'  or user_input =='thank you'):\n",
        "            continue_discussion=False\n",
        "            print(\"Prince: Most welcome\")\n",
        "        else:\n",
        "            if(reply_greeting(user_input)!=None):\n",
        "                print(\"Chatbot: \"+reply_greeting(user_input))\n",
        "            else:\n",
        "                print(\"Prince: \",end=\"\")\n",
        "                print(give_reply(user_input))\n",
        "                sentence_list.remove(user_input)\n",
        "    else:\n",
        "        continue_discussion=False\n",
        "        print(\"Prince: Take care, bye ..\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}